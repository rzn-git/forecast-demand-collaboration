{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import h3\n",
    "from scipy.sparse import lil_matrix\n",
    "from torch_geometric.utils import from_scipy_sparse_matrix\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.utils.data import DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Filtered data for top n stores\n",
      "        date locality_type product_name  product_count   latitude   longitude  \\\n",
      "1 2021-01-01       Diamond         Beta              1  40.858416  -73.781928   \n",
      "2 2021-01-01       Diamond        Alpha              1  33.363745 -118.424787   \n",
      "4 2021-01-01       Diamond        Alpha              1  30.769735  -91.458505   \n",
      "5 2021-01-01       Diamond        Gamma              1  41.274267  -81.993373   \n",
      "6 2021-01-01       Diamond        Delta              1  29.967208  -97.319137   \n",
      "\n",
      "   store_id  \n",
      "1         2  \n",
      "2         3  \n",
      "4         5  \n",
      "5         6  \n",
      "6         7  \n",
      "[DEBUG] Shape of filtered data\n",
      "(6401132, 7)\n"
     ]
    }
   ],
   "source": [
    "# Debug Helper Function\n",
    "def debug(message, variable=None):\n",
    "    print(f\"[DEBUG] {message}\")\n",
    "    if variable is not None:\n",
    "        print(variable)\n",
    "\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('dataset/sales_4.csv')\n",
    "\n",
    "# Convert date column to datetime\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "# Select top N stores\n",
    "top_n = 100\n",
    "store_lifetime_product_count = data.groupby('store_id')['product_count'].sum()\n",
    "top_n_stores = store_lifetime_product_count.sort_values(ascending=False).head(top_n).index\n",
    "filtered_data = data[data['store_id'].isin(top_n_stores)]\n",
    "\n",
    "debug(\"Filtered data for top n stores\", filtered_data.head())\n",
    "debug(\"Shape of filtered data\", filtered_data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rznis\\AppData\\Local\\Temp\\ipykernel_16184\\1361998663.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['week'] = filtered_data['date'].dt.to_period('W').apply(lambda r: r.start_time)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Shape of weekly_data after aggregation\n",
      "(20448, 6)\n"
     ]
    }
   ],
   "source": [
    "# Aggregate data to weekly demand per store\n",
    "filtered_data['week'] = filtered_data['date'].dt.to_period('W').apply(lambda r: r.start_time)\n",
    "weekly_data = filtered_data.groupby(['store_id', 'week']).agg({\n",
    "    'product_count': 'sum',\n",
    "    'latitude': 'first',\n",
    "    'longitude': 'first',\n",
    "    'locality_type': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "debug(\"Shape of weekly_data after aggregation\", weekly_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] First few rows after encoding locality_type\n",
      "   store_id       week  product_count   latitude  longitude  locality_type\n",
      "0         2 2020-12-28      -0.916058  40.858416 -73.781928              0\n",
      "1         2 2021-01-04       3.803672  40.858416 -73.781928              0\n",
      "2         2 2021-01-11       4.115463  40.858416 -73.781928              0\n",
      "3         2 2021-01-18       3.834851  40.858416 -73.781928              0\n",
      "4         2 2021-01-25       3.760801  40.858416 -73.781928              0\n",
      "[DEBUG] Shape of time_series\n",
      "(100, 205)\n"
     ]
    }
   ],
   "source": [
    "# Normalize demand\n",
    "weekly_data['product_count'] = (weekly_data['product_count'] - weekly_data['product_count'].mean()) / weekly_data['product_count'].std()\n",
    "locality_mapping = {'Diamond': 0, 'Gold': 1, 'Silver': 2}\n",
    "weekly_data['locality_type'] = weekly_data['locality_type'].map(locality_mapping)\n",
    "\n",
    "debug(\"First few rows after encoding locality_type\", weekly_data.head())\n",
    "\n",
    "time_series = weekly_data.pivot(index='store_id', columns='week', values='product_count').fillna(0)\n",
    "debug(\"Shape of time_series\", time_series.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create adjacency matrix using H3 indexing\n",
    "resolution = 3\n",
    "weekly_data['h3_index'] = weekly_data.apply(\n",
    "    lambda row: h3.latlng_to_cell(row['latitude'], row['longitude'], resolution), axis=1\n",
    ")\n",
    "h3_to_store_map = weekly_data.groupby('h3_index')['store_id'].apply(list).to_dict()\n",
    "h3_neighbors = {h: h3.grid_disk(h, 1) for h in h3_to_store_map.keys()}\n",
    "\n",
    "num_stores = len(weekly_data['store_id'].unique())\n",
    "adj_matrix = lil_matrix((num_stores, num_stores))\n",
    "store_to_idx = {store_id: idx for idx, store_id in enumerate(weekly_data['store_id'].unique())}\n",
    "\n",
    "for h3_index, neighbors in h3_neighbors.items():\n",
    "    stores_in_hex = h3_to_store_map.get(h3_index, [])\n",
    "    for neighbor in neighbors:\n",
    "        neighbor_stores = h3_to_store_map.get(neighbor, [])\n",
    "        for s1 in stores_in_hex:\n",
    "            for s2 in neighbor_stores:\n",
    "                idx1 = store_to_idx[s1]\n",
    "                idx2 = store_to_idx[s2]\n",
    "                adj_matrix[idx1, idx2] = 1\n",
    "\n",
    "adj_sparse = adj_matrix.tocsr()\n",
    "edge_index, edge_weight = from_scipy_sparse_matrix(adj_sparse)\n",
    "debug(\"Shape of edge_index\", edge_index.shape)\n",
    "debug(\"Shape of edge_weight\", edge_weight.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Prepare node features\n",
    "locality_encoded = weekly_data.groupby('store_id')['locality_type'].first()\n",
    "locality_encoded = pd.get_dummies(locality_encoded, prefix='locality_type')\n",
    "locality_tensor = torch.tensor(locality_encoded.values, dtype=torch.float)\n",
    "node_features = torch.tensor(time_series.values, dtype=torch.float)\n",
    "node_features = torch.cat([node_features, locality_tensor], dim=1)\n",
    "\n",
    "debug(\"Updated node features shape\", node_features.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the STGNN model class\n",
    "class STGNN(nn.Module):\n",
    "    def __init__(self, in_channels, spatial_out, temporal_out, time_steps, forecast_steps):\n",
    "        super(STGNN, self).__init__()\n",
    "        self.gcn = GCNConv(in_channels, spatial_out)\n",
    "        self.temporal_conv = nn.Conv1d(temporal_out, temporal_out, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.flatten_out_size = None\n",
    "        self.forecast_steps = forecast_steps\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        # Debug: Print input shapes\n",
    "        print(f\"Input x shape: {x.shape}\")  # (batch_size, num_nodes, time_steps)\n",
    "        print(f\"Edge index shape: {edge_index.shape}\")\n",
    "        print(f\"Edge weight shape: {edge_weight.shape}\")\n",
    "        \n",
    "        batch_size, num_nodes, in_channels = x.shape\n",
    "        \n",
    "        # Reshape for GCN (GCN expects x as [num_nodes, in_channels])\n",
    "        x = x.view(-1, in_channels)  # Combine batch_size and num_nodes\n",
    "        \n",
    "        # Spatial convolution (GCN)\n",
    "        x = self.gcn(x, edge_index, edge_weight)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        # Debug: Print shape after GCN\n",
    "        print(f\"x shape after GCN: {x.shape}\")  # Should be [batch_size * num_nodes, spatial_out]\n",
    "        \n",
    "        # Reshape back for temporal processing\n",
    "        x = x.view(batch_size, num_nodes, -1).permute(0, 2, 1)  # (batch_size, spatial_out, num_nodes)\n",
    "        \n",
    "        # Debug: Print shape before temporal convolution\n",
    "        print(f\"x shape before temporal convolution: {x.shape}\")\n",
    "        \n",
    "        # Temporal convolution\n",
    "        x = self.temporal_conv(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        # Debug: Print shape after temporal convolution\n",
    "        print(f\"x shape after temporal convolution: {x.shape}\")\n",
    "        \n",
    "        # Flatten dynamically\n",
    "        x = x.view(batch_size, -1)  # Flatten: (batch_size, flattened_features)\n",
    "        \n",
    "        if self.flatten_out_size is None:\n",
    "            self.flatten_out_size = x.size(1)\n",
    "            self.fc = nn.Linear(self.flatten_out_size, self.forecast_steps)\n",
    "\n",
    "        # Final prediction\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # Debug: Print output shape\n",
    "        print(f\"Output x shape: {x.shape}\")\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Dataset Class\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, time_steps, forecast_steps):\n",
    "        self.data = data\n",
    "        self.time_steps = time_steps\n",
    "        self.forecast_steps = forecast_steps\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[1] - self.time_steps - self.forecast_steps\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[:, idx:idx + self.time_steps]\n",
    "        y = self.data[:, idx + self.time_steps:idx + self.time_steps + self.forecast_steps]\n",
    "        return torch.tensor(x, dtype=torch.float), torch.tensor(y, dtype=torch.float)\n",
    "\n",
    "\n",
    "# Training setup\n",
    "time_steps = 8\n",
    "forecast_steps = 1\n",
    "spatial_out = 32\n",
    "temporal_out = 64\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "learning_rate = 0.001\n",
    "\n",
    "dataset = TimeSeriesDataset(node_features, time_steps, forecast_steps)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = STGNN(\n",
    "    in_channels=node_features.shape[1],\n",
    "    spatial_out=spatial_out,\n",
    "    temporal_out=temporal_out,\n",
    "    forecast_steps=forecast_steps\n",
    ")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Debug: Print shapes of x and y\n",
    "        print(f\"Batch x shape before reshape: {x.shape}\")\n",
    "        print(f\"Batch y shape before reshape: {y.shape}\")\n",
    "\n",
    "        # Reshape x to match model input expectations\n",
    "        batch_size, num_nodes, time_steps = x.shape\n",
    "        x = x.permute(0, 2, 1)  # (batch_size, time_steps, num_nodes)\n",
    "        \n",
    "        # Forward pass\n",
    "        out = model(x, edge_index, edge_weight)\n",
    "\n",
    "        # Adjust target shape to match output\n",
    "        y = y.view(batch_size, -1)  # Flatten y dynamically\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    debug(f\"Epoch {epoch + 1}/{epochs} Loss\", total_loss / len(dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x = node_features[:, -time_steps:]\n",
    "    x = x.unsqueeze(0)\n",
    "    prediction = model(x, edge_index, edge_weight)\n",
    "    debug(\"Prediction\", prediction)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
